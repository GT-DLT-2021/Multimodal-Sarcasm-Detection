{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4074d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (3.10.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (1.21.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (2.7.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (21.0)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (1.10.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (2021.11.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.41.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.3.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (58.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
      "Requirement already satisfied: aiohttp; extra == \"http\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: six>=1.5.2 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from grpcio>=1.24.3->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: transformers in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (4.12.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: filelock in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (21.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex!=2019.12.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: sacremoses in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: requests in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: joblib in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning #torch==1.3.1\n",
    "!pip install transformers\n",
    "!pip3 -q install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "117b4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pandas_path  # Path style access for pandas\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch                    \n",
    "import torchvision\n",
    "import fasttext\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_lists(path):\n",
    "    data_points_lists = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            try:\n",
    "                data_points_lists.append(ast.literal_eval(line))\n",
    "            except:\n",
    "                # Ignore lines with errors\n",
    "                pass\n",
    "\n",
    "    print('Found {} lines in \"{}\".'.format(len(lines), path))\n",
    "    print('Successfully loaded {} data points from \"{}\".'.format(len(data_points_lists), path))\n",
    "    \n",
    "    return data_points_lists\n",
    "\n",
    "COLUMN_NAMES = ['ID', 'Text', 'Sarcastic']\n",
    "\n",
    "def construct_df(data_points_lists, column_names=COLUMN_NAMES):\n",
    "    df = pd.DataFrame(data_points_lists, columns=column_names)\n",
    "    df['ID'] = pd.to_numeric(df['ID'])\n",
    "    df['Sarcastic'] = df['Sarcastic'].astype('bool')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1572ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29040 lines in \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/train.txt\".\n",
      "Successfully loaded 29040 data points from \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/train.txt\".\n",
      "(29040, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29040 entries, 0 to 29039\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         29040 non-null  int64 \n",
      " 1   Text       29040 non-null  object\n",
      " 2   Sarcastic  29040 non-null  bool  \n",
      "dtypes: bool(1), int64(1), object(1)\n",
      "memory usage: 482.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910308516510011393</td>\n",
       "      <td>most # funny quotes : 21 snarky and # funny qu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>725333760762363905</td>\n",
       "      <td>spurs # creativethinking ! &lt;url&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>840006160660983809</td>\n",
       "      <td>&lt;user&gt; thanks for showing up for our appointme...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>854334602516733952</td>\n",
       "      <td>only a hardcore fan of sir jonny sins will get...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>908913372199915520</td>\n",
       "      <td>haha .  # lol</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "0  910308516510011393  most # funny quotes : 21 snarky and # funny qu...   \n",
       "1  725333760762363905                   spurs # creativethinking ! <url>   \n",
       "2  840006160660983809  <user> thanks for showing up for our appointme...   \n",
       "3  854334602516733952  only a hardcore fan of sir jonny sins will get...   \n",
       "4  908913372199915520                                      haha .  # lol   \n",
       "\n",
       "   Sarcastic  \n",
       "0       True  \n",
       "1       True  \n",
       "2       True  \n",
       "3       True  \n",
       "4       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = construct_df(load_data_lists('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/train.txt'))\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train.info())\n",
    "\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0248226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2410 lines in \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/valid.txt\".\n",
      "Successfully loaded 2410 data points from \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/valid.txt\".\n",
      "(2410, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2410 entries, 0 to 2409\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   ID               2410 non-null   int64 \n",
      " 1   Text             2410 non-null   object\n",
      " 2   Sarcastic        2410 non-null   bool  \n",
      " 3   Sarcastic_human  2410 non-null   int64 \n",
      "dtypes: bool(1), int64(2), object(1)\n",
      "memory usage: 59.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sarcastic</th>\n",
       "      <th>Sarcastic_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>915657464401580032</td>\n",
       "      <td>whew ... that extra &lt;num&gt; miles today to the g...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>854678856724340736</td>\n",
       "      <td>\" oh , good . now no one will know we 're here...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>904892917277274112</td>\n",
       "      <td>how much of it you think is true ? has this be...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>855466461296504832</td>\n",
       "      <td>&lt;user&gt; finally found proof that the earth is f...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>927373534652805120</td>\n",
       "      <td>many ways to overcome tension &amp; fear but nothi...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "0  915657464401580032  whew ... that extra <num> miles today to the g...   \n",
       "1  854678856724340736  \" oh , good . now no one will know we 're here...   \n",
       "2  904892917277274112  how much of it you think is true ? has this be...   \n",
       "3  855466461296504832  <user> finally found proof that the earth is f...   \n",
       "4  927373534652805120  many ways to overcome tension & fear but nothi...   \n",
       "\n",
       "   Sarcastic  Sarcastic_human  \n",
       "0       True                1  \n",
       "1       True                1  \n",
       "2       True                1  \n",
       "3       True                1  \n",
       "4       True                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = construct_df(load_data_lists('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/valid.txt'), COLUMN_NAMES + ['Sarcastic_human'])\n",
    "print(df_val.shape)\n",
    "print(df_val.info())\n",
    "\n",
    "display(df_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac1aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2409 lines in \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/test.txt\".\n",
      "Successfully loaded 2409 data points from \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/test.txt\".\n",
      "(2409, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2409 entries, 0 to 2408\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   ID               2409 non-null   int64 \n",
      " 1   Text             2409 non-null   object\n",
      " 2   Sarcastic        2409 non-null   bool  \n",
      " 3   Sarcastic_human  2409 non-null   int64 \n",
      "dtypes: bool(1), int64(2), object(1)\n",
      "memory usage: 58.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sarcastic</th>\n",
       "      <th>Sarcastic_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862902619928506372</td>\n",
       "      <td>i am guessing # netflix no longer lets you gra...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>892551658487631873</td>\n",
       "      <td>it 's the insensitive strikeouts at suntrust p...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>853143461360480256</td>\n",
       "      <td>following the path of the river calder , so .....</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>918423568823840768</td>\n",
       "      <td># westernsahara # authority has no lessons 2ge...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731617467718610944</td>\n",
       "      <td>hey &lt;user&gt; great sale !</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "0  862902619928506372  i am guessing # netflix no longer lets you gra...   \n",
       "1  892551658487631873  it 's the insensitive strikeouts at suntrust p...   \n",
       "2  853143461360480256  following the path of the river calder , so .....   \n",
       "3  918423568823840768  # westernsahara # authority has no lessons 2ge...   \n",
       "4  731617467718610944                           hey <user> great sale !    \n",
       "\n",
       "   Sarcastic  Sarcastic_human  \n",
       "0       True                1  \n",
       "1       True                1  \n",
       "2       True                1  \n",
       "3       True                1  \n",
       "4       True                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = construct_df(load_data_lists('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/test.txt'), COLUMN_NAMES + ['Sarcastic_human'])\n",
    "print(df_test.shape)\n",
    "print(df_test.info())\n",
    "\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0079d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subdirectories\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train_dir = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/'\n",
    "val_dir = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/'\n",
    "test_dir = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/'\n",
    "\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(val_dir)\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1d185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = []\n",
    "\n",
    "for id in df_train['ID']:\n",
    "    img_path = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/{}.jpg'.format(id)\n",
    "    #img_path = 'dataset_image/{}.jpg'.format(id)\n",
    "    if os.path.isfile(img_path):\n",
    "        #shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}.jpg'.format(id))\n",
    "\n",
    "      shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}.jpg'.format(id))\n",
    "    else:\n",
    "        missing_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "964b156c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9224"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d6eea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19816, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[~df_train['ID'].isin(missing_ids)]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "243a0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in df_val['ID']:\n",
    "    img_path = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/{}.jpg'.format(id)\n",
    "    if os.path.isfile(img_path):\n",
    "      shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/{}.jpg'.format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0a17319",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in df_test['ID']:\n",
    "    img_path = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/{}.jpg'.format(id)\n",
    "    if os.path.isfile(img_path):\n",
    "      shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/{}.jpg'.format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d9e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/non-sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/non-sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/non-sarc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5272b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_foldername = {\n",
    "    True: 'sarc',\n",
    "    False: 'non-sarc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4669d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move images to subfolders\n",
    "for id,label in zip(df_train['ID'], df_train['Sarcastic']):\n",
    "    shutil.move('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}.jpg'.format(id), '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}/{}.jpg'.format(class_foldername[label], id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c4bb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move images to subfolders\n",
    "for id,label in zip(df_val['ID'], df_val['Sarcastic']):\n",
    "    shutil.move('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/{}.jpg'.format(id), '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/{}/{}.jpg'.format(class_foldername[label], id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aeffa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    datasets.ImageFolder('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train', data_transforms['train']),\n",
    "    'validation': \n",
    "    datasets.ImageFolder('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val', data_transforms['validation'])\n",
    "}\n",
    "\n",
    "image_dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=64,\n",
    "                                shuffle=True),\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=64,\n",
    "                                shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d82f739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for txt data\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import (get_linear_schedule_with_warmup,AdamW,AutoModel, AutoTokenizer,\n",
    "                            AutoModelForSequenceClassification)\n",
    "from torch.utils.data import (TensorDataset,DataLoader,\n",
    "                             RandomSampler, SequentialSampler, Dataset)\n",
    "\n",
    "\n",
    "train_df = shuffle(df_train, random_state=42)\n",
    "valid_df = shuffle(df_val, random_state=42)\n",
    "test_df = shuffle(df_test, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d69b2124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "text_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-base\",\n",
    "        num_labels = 2,  \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86d8cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(df, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    # print(df)\n",
    "    for sent in df[['Text']].values:\n",
    "        sent = sent.item()\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = 128,           \n",
    "                            pad_to_max_length = True,\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   \n",
    "                            return_tensors = 'pt',    \n",
    "                    )\n",
    "           \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    inputs = {\n",
    "    'input_word_ids': input_ids,\n",
    "    'input_mask': attention_masks}\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25b6d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset,DataLoader,\n",
    "                             RandomSampler, SequentialSampler, Dataset)\n",
    "\n",
    "def prepare_dataloaders(train_df,test_df,batch_size=64):\n",
    "    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "    \n",
    "    tweet_train = bert_encode(train_df, tokenizer)\n",
    "    tweet_train_labels = train_df['Sarcastic'].astype(int)\n",
    "    \n",
    "    tweet_test = bert_encode(test_df, tokenizer)\n",
    "\n",
    "    input_ids, attention_masks = tweet_train.values()\n",
    "    labels = torch.tensor(tweet_train_labels.values)\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    \n",
    "    input_ids, attention_masks = tweet_test.values()\n",
    "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset), \n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset, \n",
    "                sampler = SequentialSampler(test_dataset), \n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d89a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "text_train_dataloader,text_test_dataloader = prepare_dataloaders(train_df, test_df)\n",
    "_,text_val_dataloader = prepare_dataloaders(train_df, valid_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39749b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59729190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "  def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
    "    super(CNN_LSTM, self).__init__()\n",
    "\n",
    "    # LSTM for the text overview\n",
    "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "    self.emb.weight.requires_grad = True\n",
    "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # CNN for the posters\n",
    "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "    self.max_pool1 = nn.MaxPool2d(2)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "    self.max_pool2 = nn.MaxPool2d(2)\n",
    "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "    self.max_pool3 = nn.MaxPool2d(2)\n",
    "    self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "    self.max_pool4 = nn.MaxPool2d(2)\n",
    "    self.cnn_dropout = nn.Dropout(0.1)\n",
    "    self.cnn_fc = nn.Linear(5*2*128, 512)\n",
    "    # Concat layer for the combined feature space\n",
    "    self.combined_fc1 = nn.Linear(640, 256)\n",
    "    self.combined_fc2 = nn.Linear(256, 128)\n",
    "    self.output_fc = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "  def forward(self, lstm_inp, cnn_inp):\n",
    "    batch_size = lstm_inp.size(0)\n",
    "    hidden = self.init_hidden(batch_size)\n",
    "    lstm_inp = lstm_inp.long()\n",
    "    embeds = self.emb(lstm_inp)\n",
    "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    lstm_out = self.dropout(lstm_out[:, -1])\n",
    "    lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "\n",
    "    x = F.relu(self.conv1(cnn_inp))\n",
    "    x = self.max_pool1(x)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = self.max_pool2(x)\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = self.max_pool3(x)\n",
    "    x = F.relu(self.conv4(x))\n",
    "    x = self.max_pool4(x)\n",
    "    x = x.view(-1, 5*2*128)\n",
    "    x = self.cnn_dropout(x)\n",
    "    cnn_out = F.relu(self.cnn_fc(x))\n",
    "    combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "    x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "    x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "    out = torch.sigmoid(self.output_fc(x_comb))\n",
    "\n",
    "    return out\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6badc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 20\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "  total_acc_train = 0\n",
    "  total_loss_train = 0\n",
    "    \n",
    "  for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
    "    lstm_inp, lstm_labels = lstm\n",
    "    cnn_inp, cnn_labels = cnn\n",
    "    lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "    cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "    model.zero_grad()\n",
    "    output = model(lstm_inp, cnn_inp)\n",
    "    loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "      acc = (1. - acc.sum() / acc.size()[0])\n",
    "      total_acc_train += acc\n",
    "      total_loss_train += loss.item()\n",
    "  \n",
    "  train_acc = total_acc_train/len(text_train_loader)\n",
    "  train_loss = total_loss_train/len(text_train_loader)\n",
    "  model.eval()\n",
    "  total_acc_val = 0\n",
    "  total_loss_val = 0\n",
    "  with torch.no_grad():\n",
    "    for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
    "      lstm_inp, lstm_labels = lstm\n",
    "      cnn_inp, cnn_labels = cnn\n",
    "      lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "      cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "      model.zero_grad()\n",
    "      output = model(lstm_inp, cnn_inp)\n",
    "      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "      acc = (1. - acc.sum() / acc.size()[0])\n",
    "      total_acc_val += acc\n",
    "      total_loss_val += val_loss.item()\n",
    "    print(\"Saving model...\") \n",
    "    torch.save(model.state_dict(), '/content/drive/My Drive/Movie_genre_prediction_dataset/pytorch_word2vec_lstm_less_dropout.pt')\n",
    "\n",
    "  val_acc = total_acc_val/len(text_val_loader)\n",
    "  val_loss = total_loss_val/len(text_val_loader)\n",
    "  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "  model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3e123e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_class=\"vinai/bertweet-base\",num_classes=2,model_to_load=None,total_steps=-1):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_class,\n",
    "        num_labels = num_classes,  \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = True,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 5e-5,\n",
    "                    eps = 1e-8\n",
    "                    )\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    if model_to_load is not None:\n",
    "        try:\n",
    "            model.roberta.load_state_dict(torch.load(model_to_load))\n",
    "            print(\"LOADED MODEL\")\n",
    "        except:\n",
    "            pass\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4e8ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModel, self).__init__()\n",
    "        #Resnet50 for image\n",
    "        self.text_model, _, _ = prepare_model(\"vinai/bertweet-base\" ,num_classes=2, model_to_load=None, total_steps = total_steps)\n",
    "        self.linear_1 = nn.linear(768,300)\n",
    "        #self.text_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        #model_class = \"vinai/bertweet-base\",\n",
    "        #num_labels = 2,  \n",
    "        #output_attentions = False, \n",
    "        #output_hidden_states = False,)\n",
    "        \n",
    "        self.image_model = torchvision.models.resnet50(\n",
    "            pretrained=True\n",
    "        )\n",
    "        #resnet_output_feature_dim = 300\n",
    "        #bert_output_feature_dim = 300\n",
    "        #fusion_input_feature = 300+300 = 600\n",
    "        self.image_model.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 300))\n",
    "        \n",
    "        \n",
    "        # Combined\n",
    "        \n",
    "        #fusion_input_feature = 300+300 = 600\n",
    "        #fusion_output_size = 128\n",
    "        self.combined_fc1 = nn.Linear(600, 256)\n",
    "        self.combined_fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.output_fc = nn.Linear(128, 2)\n",
    "       \n",
    "        #self.text_train_dataset = image_dataloaders[\"train\"]\n",
    "        #self.text_dev_dataset = image_dataloaders[\"validation\"]\n",
    "        #self.image_train_dataset = text_train_dataloader\n",
    "        #self.image_dev_dataset = text_val_dataloader\n",
    "\n",
    "    \n",
    "    def forward(self, text,text_mask,image):\n",
    "        text = text\n",
    "        bert_out = self.text_model(text)\n",
    "        last_layer = bert_out\n",
    "        #print(len(bert_out))\n",
    "        \n",
    "        #last_layer = bert_out.hidden_states[-1]\n",
    "        #print(last_layer.shape)\n",
    "        image = image\n",
    "        resnet_out = self.image_model(image)\n",
    "        print(\"resnet output\")\n",
    "        print(resnet_out)\n",
    "        combined_emb = torch.cat((resnet_out, last_layer), 1)\n",
    "        x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "        x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "        out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        #logits = self.output_fc(x_comb)\n",
    "        #pred = torch.nn.functional.softmax(logits)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bf902f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6d/pg34ylzj4mn7fp70wgtbx64w0000gn/T/ipykernel_11414/3041031828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#count = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_dataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \"\"\"\n\u001b[1;32m    231\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \"\"\"\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#count = 0\n",
    "for image,label in image_dataloaders[\"train\"]:\n",
    "    count +=1\n",
    "    #print(image)\n",
    "    #print(label)\n",
    "    if count ==5:\n",
    "        break\n",
    "count=0\n",
    "for batch in text_train_dataloader:\n",
    "    count +=1\n",
    "    #print(text)\n",
    " #   print(len(batch))\n",
    "    text = batch[0]\n",
    "    label = batch[1]\n",
    "  #  print(text.shape)\n",
    "   # print(label.shape)\n",
    "    if count==5:\n",
    "        break\n",
    "count = 0\n",
    "#print(\"haha\")\n",
    "for batch, [image,label] in zip(text_train_dataloader,image_dataloaders[\"train\"]):\n",
    "    count+=1\n",
    " #   print(batch[0].shape)\n",
    "  #  print(batch[1].shape)\n",
    "   # print(batch[2])\n",
    "    #print(image.shape)\n",
    "    #print(label.shape)\n",
    "    if count==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e1e3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, image_data_loaders,text_train_data_loader,text_val_data_loader, criterion, optimizer, num_epochs=3):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "        \n",
    "            for batch,[image_inputs,image_labels] in zip(text_train_data_loader,image_data_loaders[phase]):\n",
    "                text_inputs = batch[0]\n",
    "                text_mask = batch[1]\n",
    "                text_labels = batch[2]\n",
    "                text_inputs = text_inputs.to(device)\n",
    "                text_labels = text_labels.to(device)\n",
    "                image_inputs = image_inputs.to(device)\n",
    "                image_labels = image_labels.to(device)\n",
    "                outputs = model(text_inputs,text_mask,image_inputs)\n",
    "                \n",
    "                loss = criterion(outputs, image_labels)\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fcfda52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "total_steps = len(train_df) * epochs\n",
    "model = MultiModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                    lr = 5e-5,\n",
    "                    eps = 1e-8\n",
    "                    )\n",
    "\n",
    "#loss_func = nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59b1f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "2\n",
      "torch.Size([64, 128, 768])\n",
      "resnet output\n",
      "tensor([[-0.0474,  0.1222],\n",
      "        [ 0.0280,  0.0150],\n",
      "        [-0.0980,  0.1566],\n",
      "        [ 0.0490, -0.0579],\n",
      "        [-0.0842, -0.0290],\n",
      "        [-0.1612,  0.0745],\n",
      "        [-0.0963,  0.1839],\n",
      "        [-0.2831,  0.0303],\n",
      "        [ 0.1437,  0.0031],\n",
      "        [-0.0885, -0.0522],\n",
      "        [-0.0937, -0.0756],\n",
      "        [-0.0884,  0.0158],\n",
      "        [-0.0973,  0.0659],\n",
      "        [-0.0673, -0.0121],\n",
      "        [-0.0483, -0.0693],\n",
      "        [-0.0863, -0.0393],\n",
      "        [-0.0081,  0.1018],\n",
      "        [ 0.0427, -0.0053],\n",
      "        [ 0.0268,  0.0669],\n",
      "        [-0.2282,  0.0632],\n",
      "        [-0.1202, -0.1040],\n",
      "        [-0.1503,  0.1585],\n",
      "        [-0.0519, -0.0262],\n",
      "        [ 0.0673,  0.0800],\n",
      "        [ 0.0019, -0.0350],\n",
      "        [-0.0701, -0.0380],\n",
      "        [-0.0646, -0.0404],\n",
      "        [-0.1369, -0.0731],\n",
      "        [-0.1098, -0.0377],\n",
      "        [-0.0929,  0.0205],\n",
      "        [-0.0067, -0.0406],\n",
      "        [-0.1511,  0.0272],\n",
      "        [-0.0190,  0.0513],\n",
      "        [ 0.0484,  0.0150],\n",
      "        [-0.0117,  0.0266],\n",
      "        [-0.2370,  0.0648],\n",
      "        [-0.0503,  0.0483],\n",
      "        [ 0.0088,  0.0542],\n",
      "        [-0.0838,  0.1421],\n",
      "        [-0.0758,  0.1377],\n",
      "        [ 0.0650,  0.1654],\n",
      "        [-0.0860,  0.0182],\n",
      "        [-0.1673,  0.1309],\n",
      "        [-0.0407, -0.0282],\n",
      "        [-0.0713, -0.0632],\n",
      "        [-0.2623,  0.0006],\n",
      "        [-0.0367, -0.0339],\n",
      "        [-0.1394, -0.0753],\n",
      "        [ 0.0318,  0.0363],\n",
      "        [-0.1692, -0.0899],\n",
      "        [-0.0650, -0.0566],\n",
      "        [-0.0425, -0.1411],\n",
      "        [-0.1187,  0.0340],\n",
      "        [-0.0620,  0.0493],\n",
      "        [-0.0423, -0.0437],\n",
      "        [ 0.0453, -0.0855],\n",
      "        [-0.2161,  0.1875],\n",
      "        [-0.0847, -0.0825],\n",
      "        [-0.1729, -0.0627],\n",
      "        [-0.0904,  0.0600],\n",
      "        [-0.1385,  0.1035],\n",
      "        [-0.0336,  0.0933],\n",
      "        [-0.0364, -0.0200],\n",
      "        [-0.0641,  0.0066]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6d/pg34ylzj4mn7fp70wgtbx64w0000gn/T/ipykernel_24828/977307499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_val_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/6d/pg34ylzj4mn7fp70wgtbx64w0000gn/T/ipykernel_24828/1518525278.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, image_data_loaders, text_train_data_loader, text_val_data_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mimage_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mimage_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6d/pg34ylzj4mn7fp70wgtbx64w0000gn/T/ipykernel_24828/83308371.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_mask, image)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resnet output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mcombined_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined_fc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined_fc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_comb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model,image_dataloaders,text_train_dataloader,text_val_dataloader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37228d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83622d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/content/drive/MyDrive/Sarcasm-Detection-hierarchical_fusion_model/multimodal_10epoch.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
