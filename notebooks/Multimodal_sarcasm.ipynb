{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4074d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_lightning in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (21.0)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (1.10.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (2.7.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (2021.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (3.10.0.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (1.21.2)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pytorch_lightning) (0.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.26.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.41.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (58.0.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: aiohttp; extra == \"http\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.1)\n",
      "Requirement already satisfied: six in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from aiohttp; extra == \"http\"->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.1)\n",
      "Requirement already satisfied: transformers in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (4.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (0.1.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (2021.11.10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: requests in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: sacremoses in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (1.21.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning #torch==1.3.1\n",
    "!pip install transformers\n",
    "!pip3 -q install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117b4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pandas_path  # Path style access for pandas\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch                    \n",
    "import torchvision\n",
    "import fasttext\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f227d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_lists(path):\n",
    "    data_points_lists = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            try:\n",
    "                data_points_lists.append(ast.literal_eval(line))\n",
    "            except:\n",
    "                # Ignore lines with errors\n",
    "                pass\n",
    "\n",
    "    print('Found {} lines in \"{}\".'.format(len(lines), path))\n",
    "    print('Successfully loaded {} data points from \"{}\".'.format(len(data_points_lists), path))\n",
    "    \n",
    "    return data_points_lists\n",
    "\n",
    "COLUMN_NAMES = ['ID', 'Text', 'Sarcastic']\n",
    "\n",
    "def construct_df(data_points_lists, column_names=COLUMN_NAMES):\n",
    "    df = pd.DataFrame(data_points_lists, columns=column_names)\n",
    "    df['ID'] = pd.to_numeric(df['ID'])\n",
    "    df['Sarcastic'] = df['Sarcastic'].astype('bool')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1572ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29040 lines in \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/train.txt\".\n",
      "Successfully loaded 29040 data points from \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/train.txt\".\n",
      "(29040, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29040 entries, 0 to 29039\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ID         29040 non-null  int64 \n",
      " 1   Text       29040 non-null  object\n",
      " 2   Sarcastic  29040 non-null  bool  \n",
      "dtypes: bool(1), int64(1), object(1)\n",
      "memory usage: 482.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910308516510011393</td>\n",
       "      <td>most # funny quotes : 21 snarky and # funny qu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>725333760762363905</td>\n",
       "      <td>spurs # creativethinking ! &lt;url&gt;</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>840006160660983809</td>\n",
       "      <td>&lt;user&gt; thanks for showing up for our appointme...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>854334602516733952</td>\n",
       "      <td>only a hardcore fan of sir jonny sins will get...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>908913372199915520</td>\n",
       "      <td>haha .  # lol</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "0  910308516510011393  most # funny quotes : 21 snarky and # funny qu...   \n",
       "1  725333760762363905                   spurs # creativethinking ! <url>   \n",
       "2  840006160660983809  <user> thanks for showing up for our appointme...   \n",
       "3  854334602516733952  only a hardcore fan of sir jonny sins will get...   \n",
       "4  908913372199915520                                      haha .  # lol   \n",
       "\n",
       "   Sarcastic  \n",
       "0       True  \n",
       "1       True  \n",
       "2       True  \n",
       "3       True  \n",
       "4       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = construct_df(load_data_lists('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/train.txt'))\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train.info())\n",
    "\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0248226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2410 lines in \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/valid.txt\".\n",
      "Successfully loaded 2410 data points from \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/valid.txt\".\n",
      "(2410, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2410 entries, 0 to 2409\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   ID               2410 non-null   int64 \n",
      " 1   Text             2410 non-null   object\n",
      " 2   Sarcastic        2410 non-null   bool  \n",
      " 3   Sarcastic_human  2410 non-null   int64 \n",
      "dtypes: bool(1), int64(2), object(1)\n",
      "memory usage: 59.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sarcastic</th>\n",
       "      <th>Sarcastic_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>915657464401580032</td>\n",
       "      <td>whew ... that extra &lt;num&gt; miles today to the g...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>854678856724340736</td>\n",
       "      <td>\" oh , good . now no one will know we 're here...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>904892917277274112</td>\n",
       "      <td>how much of it you think is true ? has this be...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>855466461296504832</td>\n",
       "      <td>&lt;user&gt; finally found proof that the earth is f...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>927373534652805120</td>\n",
       "      <td>many ways to overcome tension &amp; fear but nothi...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "0  915657464401580032  whew ... that extra <num> miles today to the g...   \n",
       "1  854678856724340736  \" oh , good . now no one will know we 're here...   \n",
       "2  904892917277274112  how much of it you think is true ? has this be...   \n",
       "3  855466461296504832  <user> finally found proof that the earth is f...   \n",
       "4  927373534652805120  many ways to overcome tension & fear but nothi...   \n",
       "\n",
       "   Sarcastic  Sarcastic_human  \n",
       "0       True                1  \n",
       "1       True                1  \n",
       "2       True                1  \n",
       "3       True                1  \n",
       "4       True                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val = construct_df(load_data_lists('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/valid.txt'), COLUMN_NAMES + ['Sarcastic_human'])\n",
    "print(df_val.shape)\n",
    "print(df_val.info())\n",
    "\n",
    "display(df_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac1aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2409 lines in \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/test.txt\".\n",
      "Successfully loaded 2409 data points from \"/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/test.txt\".\n",
      "(2409, 4)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2409 entries, 0 to 2408\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   ID               2409 non-null   int64 \n",
      " 1   Text             2409 non-null   object\n",
      " 2   Sarcastic        2409 non-null   bool  \n",
      " 3   Sarcastic_human  2409 non-null   int64 \n",
      "dtypes: bool(1), int64(2), object(1)\n",
      "memory usage: 58.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sarcastic</th>\n",
       "      <th>Sarcastic_human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862902619928506372</td>\n",
       "      <td>i am guessing # netflix no longer lets you gra...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>892551658487631873</td>\n",
       "      <td>it 's the insensitive strikeouts at suntrust p...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>853143461360480256</td>\n",
       "      <td>following the path of the river calder , so .....</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>918423568823840768</td>\n",
       "      <td># westernsahara # authority has no lessons 2ge...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731617467718610944</td>\n",
       "      <td>hey &lt;user&gt; great sale !</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "0  862902619928506372  i am guessing # netflix no longer lets you gra...   \n",
       "1  892551658487631873  it 's the insensitive strikeouts at suntrust p...   \n",
       "2  853143461360480256  following the path of the river calder , so .....   \n",
       "3  918423568823840768  # westernsahara # authority has no lessons 2ge...   \n",
       "4  731617467718610944                           hey <user> great sale !    \n",
       "\n",
       "   Sarcastic  Sarcastic_human  \n",
       "0       True                1  \n",
       "1       True                1  \n",
       "2       True                1  \n",
       "3       True                1  \n",
       "4       True                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = construct_df(load_data_lists('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/text_data/test.txt'), COLUMN_NAMES + ['Sarcastic_human'])\n",
    "print(df_test.shape)\n",
    "print(df_test.info())\n",
    "\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0079d285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subdirectories\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train_dir = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/'\n",
    "val_dir = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/'\n",
    "test_dir = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/'\n",
    "\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(val_dir)\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1d185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ids = []\n",
    "\n",
    "for id in df_train['ID']:\n",
    "    img_path = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/{}.jpg'.format(id)\n",
    "    #img_path = 'dataset_image/{}.jpg'.format(id)\n",
    "    if os.path.isfile(img_path):\n",
    "        #shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}.jpg'.format(id))\n",
    "\n",
    "      shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}.jpg'.format(id))\n",
    "    else:\n",
    "        missing_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964b156c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9224"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d6eea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19816, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[~df_train['ID'].isin(missing_ids)]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243a0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in df_val['ID']:\n",
    "    img_path = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/{}.jpg'.format(id)\n",
    "    if os.path.isfile(img_path):\n",
    "      shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/{}.jpg'.format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a17319",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in df_test['ID']:\n",
    "    img_path = '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/{}.jpg'.format(id)\n",
    "    if os.path.isfile(img_path):\n",
    "      shutil.move(img_path, '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/{}.jpg'.format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d9e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/non-sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/non-sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/sarc/')\n",
    "os.mkdir('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/test/non-sarc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5272b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_foldername = {\n",
    "    True: 'sarc',\n",
    "    False: 'non-sarc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4669d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move images to subfolders\n",
    "for id,label in zip(df_train['ID'], df_train['Sarcastic']):\n",
    "    shutil.move('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}.jpg'.format(id), '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train/{}/{}.jpg'.format(class_foldername[label], id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c4bb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move images to subfolders\n",
    "for id,label in zip(df_val['ID'], df_val['Sarcastic']):\n",
    "    shutil.move('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/{}.jpg'.format(id), '/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val/{}/{}.jpg'.format(class_foldername[label], id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeffa38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'validation':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': \n",
    "    datasets.ImageFolder('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/train', data_transforms['train']),\n",
    "    'validation': \n",
    "    datasets.ImageFolder('/Users/albert/Documents/CSE8813_DLT/Sarcasm-Detection-hierarchical_fusion_model/image_data/val', data_transforms['validation'])\n",
    "}\n",
    "\n",
    "image_dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                batch_size=64,\n",
    "                                shuffle=True),\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
    "                                batch_size=64,\n",
    "                                shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d82f739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for txt data\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import (get_linear_schedule_with_warmup,AdamW,AutoModel, AutoTokenizer,\n",
    "                            AutoModelForSequenceClassification)\n",
    "from torch.utils.data import (TensorDataset,DataLoader,\n",
    "                             RandomSampler, SequentialSampler, Dataset)\n",
    "\n",
    "\n",
    "train_df = shuffle(df_train, random_state=42)\n",
    "valid_df = shuffle(df_val, random_state=42)\n",
    "test_df = shuffle(df_test, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d69b2124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "text_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"vinai/bertweet-base\",\n",
    "        num_labels = 2,  \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86d8cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(df, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    # print(df)\n",
    "    for sent in df[['Text']].values:\n",
    "        sent = sent.item()\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      \n",
    "                            add_special_tokens = True, \n",
    "                            max_length = 128,           \n",
    "                            pad_to_max_length = True,\n",
    "                            truncation = True,\n",
    "                            return_attention_mask = True,   \n",
    "                            return_tensors = 'pt',    \n",
    "                    )\n",
    "           \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    inputs = {\n",
    "    'input_word_ids': input_ids,\n",
    "    'input_mask': attention_masks}\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25b6d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset,DataLoader,\n",
    "                             RandomSampler, SequentialSampler, Dataset)\n",
    "\n",
    "def prepare_dataloaders(train_df,test_df,batch_size=64):\n",
    "    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "    \n",
    "    tweet_train = bert_encode(train_df, tokenizer)\n",
    "    tweet_train_labels = train_df['Sarcastic'].astype(int)\n",
    "    \n",
    "    tweet_test = bert_encode(test_df, tokenizer)\n",
    "\n",
    "    input_ids, attention_masks = tweet_train.values()\n",
    "    labels = torch.tensor(tweet_train_labels.values)\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "    \n",
    "    input_ids, attention_masks = tweet_test.values()\n",
    "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset), \n",
    "                batch_size = batch_size \n",
    "            )\n",
    "\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "                test_dataset, \n",
    "                sampler = SequentialSampler(test_dataset), \n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d89a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/albert/opt/anaconda3/envs/DLT_project2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "text_train_dataloader,text_test_dataloader = prepare_dataloaders(train_df, test_df)\n",
    "_,text_val_dataloader = prepare_dataloaders(train_df, valid_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39749b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59729190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "  def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
    "    super(CNN_LSTM, self).__init__()\n",
    "\n",
    "    # LSTM for the text overview\n",
    "    self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "    self.emb.weight.requires_grad = True\n",
    "    self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # CNN for the posters\n",
    "    self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "    self.max_pool1 = nn.MaxPool2d(2)\n",
    "    self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "    self.max_pool2 = nn.MaxPool2d(2)\n",
    "    self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "    self.max_pool3 = nn.MaxPool2d(2)\n",
    "    self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "    self.max_pool4 = nn.MaxPool2d(2)\n",
    "    self.cnn_dropout = nn.Dropout(0.1)\n",
    "    self.cnn_fc = nn.Linear(5*2*128, 512)\n",
    "    # Concat layer for the combined feature space\n",
    "    self.combined_fc1 = nn.Linear(640, 256)\n",
    "    self.combined_fc2 = nn.Linear(256, 128)\n",
    "    self.output_fc = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "  def forward(self, lstm_inp, cnn_inp):\n",
    "    batch_size = lstm_inp.size(0)\n",
    "    hidden = self.init_hidden(batch_size)\n",
    "    lstm_inp = lstm_inp.long()\n",
    "    embeds = self.emb(lstm_inp)\n",
    "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    lstm_out = self.dropout(lstm_out[:, -1])\n",
    "    lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "\n",
    "    x = F.relu(self.conv1(cnn_inp))\n",
    "    x = self.max_pool1(x)\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = self.max_pool2(x)\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = self.max_pool3(x)\n",
    "    x = F.relu(self.conv4(x))\n",
    "    x = self.max_pool4(x)\n",
    "    x = x.view(-1, 5*2*128)\n",
    "    x = self.cnn_dropout(x)\n",
    "    cnn_out = F.relu(self.cnn_fc(x))\n",
    "    combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "    x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "    x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "    out = torch.sigmoid(self.output_fc(x_comb))\n",
    "\n",
    "    return out\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6badc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch = 20\n",
    "#model.train()\n",
    "#for i in range(epochs):\n",
    "#  total_acc_train = 0\n",
    "#  total_loss_train = 0\n",
    "    \n",
    "#  for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
    "#    lstm_inp, lstm_labels = lstm\n",
    "#    cnn_inp, cnn_labels = cnn\n",
    "#    lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "#    cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "#    model.zero_grad()\n",
    "#    output = model(lstm_inp, cnn_inp)\n",
    "#    loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "#    loss.backward()\n",
    "#    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "#    optimizer.step()\n",
    "    \n",
    "#    with torch.no_grad():\n",
    "#      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "#      acc = (1. - acc.sum() / acc.size()[0])\n",
    "#      total_acc_train += acc\n",
    "#      total_loss_train += loss.item()\n",
    "  \n",
    "#  train_acc = total_acc_train/len(text_train_loader)\n",
    "#  train_loss = total_loss_train/len(text_train_loader)\n",
    "#  model.eval()\n",
    "#  total_acc_val = 0\n",
    "#  total_loss_val = 0\n",
    "#  with torch.no_grad():\n",
    "#    for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
    "#      lstm_inp, lstm_labels = lstm\n",
    "#      cnn_inp, cnn_labels = cnn\n",
    "#      lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "#      cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "#      model.zero_grad()\n",
    "#      output = model(lstm_inp, cnn_inp)\n",
    "#      val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "#      acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "#      acc = (1. - acc.sum() / acc.size()[0])\n",
    "#      total_acc_val += acc\n",
    "#      total_loss_val += val_loss.item()\n",
    "#    print(\"Saving model...\") \n",
    "#    torch.save(model.state_dict(), dropout.pt')\n",
    "\n",
    "#  val_acc = total_acc_val/len(text_val_loader)\n",
    "#  val_loss = total_loss_val/len(text_val_loader)\n",
    "#  print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "#  model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3e123e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_class=\"vinai/bertweet-base\",num_classes=2,model_to_load=None,total_steps=-1):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_class,\n",
    "        num_labels = num_classes,  \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = True,\n",
    "    )\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                    lr = 5e-5,\n",
    "                    eps = 1e-8\n",
    "                    )\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, \n",
    "                                                num_training_steps = total_steps)\n",
    "\n",
    "    if model_to_load is not None:\n",
    "        try:\n",
    "            model.roberta.load_state_dict(torch.load(model_to_load))\n",
    "            print(\"LOADED MODEL\")\n",
    "        except:\n",
    "            pass\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4e8ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiModel, self).__init__()\n",
    "        #Resnet50 for image\n",
    "        self.text_model, _, _ = prepare_model(\"vinai/bertweet-base\" ,num_classes=2, model_to_load=None, total_steps = total_steps)\n",
    "        self.linear_1 = nn.Linear(768,300)\n",
    "        #self.text_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        #model_class = \"vinai/bertweet-base\",\n",
    "        #num_labels = 2,  \n",
    "        #output_attentions = False, \n",
    "        #output_hidden_states = False,)\n",
    "        \n",
    "        self.image_model = torchvision.models.resnet50(\n",
    "            pretrained=True\n",
    "        )\n",
    "        #resnet_output_feature_dim = 300\n",
    "        #bert_output_feature_dim = 300\n",
    "        #fusion_input_feature = 300+300 = 600\n",
    "        self.image_model.fc = nn.Sequential(\n",
    "               nn.Linear(2048, 128),\n",
    "               nn.ReLU(inplace=True),\n",
    "               nn.Linear(128, 300))\n",
    "        \n",
    "        \n",
    "        # Combined\n",
    "        \n",
    "        #fusion_input_feature = 300+300 = 600\n",
    "        #fusion_output_size = 128\n",
    "        self.combined_fc1 = nn.Linear(600, 256)\n",
    "        self.combined_fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.output_fc = nn.Linear(128, 2)\n",
    "       \n",
    "        #self.text_train_dataset = image_dataloaders[\"train\"]\n",
    "        #self.text_dev_dataset = image_dataloaders[\"validation\"]\n",
    "        #self.image_train_dataset = text_train_dataloader\n",
    "        #self.image_dev_dataset = text_val_dataloader\n",
    "\n",
    "    \n",
    "    def forward(self, text,text_mask,image):\n",
    "        text = text\n",
    "        bert_out = self.text_model(text)\n",
    "        last_layer = bert_out\n",
    "        #print(len(bert_out))\n",
    "        \n",
    "        last_layer = bert_out.hidden_states[-1][:, 0, :]\n",
    "        print(last_layer.shape)\n",
    "        image = image\n",
    "        resnet_out = self.image_model(image)\n",
    "        print(\"resnet output\")\n",
    "        print(resnet_out.shape)\n",
    "        last_layer= self.linear_1(last_layer)\n",
    "        combined_emb = torch.cat((resnet_out, last_layer), 1)\n",
    "        print(\"combined_emb\")\n",
    "        print(combined_emb.shape)\n",
    "        x_comb = F.relu(self.combined_fc1(combined_emb))\n",
    "        x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "        out = torch.sigmoid(self.output_fc(x_comb))\n",
    "        #logits = self.output_fc(x_comb)\n",
    "        #pred = torch.nn.functional.softmax(logits)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bf902f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0\n",
    "#for batch, [image,label] in zip(text_train_dataloader,image_dataloaders[\"train\"]):\n",
    "#    count+=1\n",
    " #   print(batch[0].shape)\n",
    "  #  print(batch[1].shape)\n",
    "   # print(batch[2])\n",
    "    #print(image.shape)\n",
    "    #print(label.shape)\n",
    "#    if count==5:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e1e3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, image_data_loaders,text_train_data_loader,text_val_data_loader, criterion, optimizer, num_epochs=3):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "        \n",
    "            for batch,[image_inputs,image_labels] in zip(text_train_data_loader,image_data_loaders[phase]):\n",
    "                text_inputs = batch[0]\n",
    "                text_mask = batch[1]\n",
    "                text_labels = batch[2]\n",
    "                text_inputs = text_inputs.to(device)\n",
    "                text_labels = text_labels.to(device)\n",
    "                image_inputs = image_inputs.to(device)\n",
    "                image_labels = image_labels.to(device)\n",
    "                outputs = model(text_inputs,text_mask,image_inputs)\n",
    "                \n",
    "                loss = criterion(outputs, image_labels)\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fcfda52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "total_steps = len(train_df) * epochs\n",
    "model = MultiModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                    lr = 5e-5,\n",
    "                    eps = 1e-8\n",
    "                    )\n",
    "\n",
    "#loss_func = nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "torch.Size([64, 768])\n",
      "resnet output\n",
      "torch.Size([64, 300])\n",
      "combined_emb\n",
      "torch.Size([64, 600])\n"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model,image_dataloaders,text_train_dataloader,text_val_dataloader, criterion, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37228d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83622d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/content/drive/MyDrive/Sarcasm-Detection-hierarchical_fusion_model/multimodal_10epoch.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
